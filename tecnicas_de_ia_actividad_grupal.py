# -*- coding: utf-8 -*-
"""Tecnicas_de_IA_Actividad_Grupal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ssRx90rZLwEoBBC0WfBnT7lAkvDGMG4F
"""



"""# **Universidad Internacional de la Rioja en México**
##**Maestría en Análisis y Visualización de Datos Masivos**<br>
###**Técnicas de Inteligencia Artificial**<br>
###**Actividad grupal: Análisis libre de un catálogo de datos**<br>
Presentan:<br> 
**Abdiel Pérez Ramos** Matrícula: **5256611-501798**<br>
**Alan Rai Rivera García** Matrícula: **5289974-496474**<br>
**Luís Edgardo Sosa** Matrícula: **5247093-492807**<br>
**Jorge Antonio Monge** Matrícula: **5257783-503021**<br>
Grupo: **46**<br><br> 
Profesora: **Patricia Rayón**<br><br> 
Ciudad de México, Agosto 2022
"""

import pandas as pd  #pandas
import numpy as np   #numpy 
#matplotlib
import matplotlib.pyplot as plt
import matplotlib.image as pltimg
from matplotlib.lines import Line2D
#skleark
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score 
from sklearn.model_selection import StratifiedKFold 
from sklearn import metrics
from sklearn.metrics import classification_report 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import plot_confusion_matrix
from sklearn import tree
from sklearn.metrics import RocCurveDisplay
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import pydotplus   #pydotplus
from sklearn.decomposition import PCA
from numpy import where
from sklearn.datasets import make_classification 
from matplotlib import pyplot
from numpy import unique
from numpy import where
from sklearn.cluster import KMeans,MeanShift, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering

import warnings
warnings.filterwarnings('ignore')

"""## **Importación de dataset**"""

url_europa = 'https://raw.githubusercontent.com/abidieruMX/Datasets/main/data_europa.csv'
url_vaccination = 'https://raw.githubusercontent.com/abidieruMX/Datasets/main/vaccination-data.csv'
url_pib = 'https://raw.githubusercontent.com/abidieruMX/Datasets/main/PIB.csv'

data_europa = pd.read_csv(url_europa,sep=',')
data_vaccination = pd.read_csv(url_vaccination, sep=',')
data_pib = pd.read_csv(url_pib, sep=',')

"""Se realiza una primer vistazo a los datos"""

data_europa.head()

data_vaccination.head()

data_pib.head()

"""Acotación de datos de Covid para el año 2021"""

data_europa = data_europa[data_europa.year==2021]

"""Se realiza una revisión de los tipos de datos y la integridad de estos"""

data_europa.info()

data_vaccination.info()

"""Eliminamos datos innecesarios"""

data_europa=data_europa.drop(['dateRep','day','month','year',],axis=1)

data_vaccination= data_vaccination.drop(['DATA_SOURCE','DATE_UPDATED','FIRST_VACCINE_DATE','PERSONS_BOOSTER_ADD_DOSE','PERSONS_BOOSTER_ADD_DOSE_PER100'], axis=1)

data_pib=data_pib[['Country Name','2021','Country Code']]

data_pib=data_pib[data_pib['Country Name']!=0]

"""Rellenamos los NAs con ceros, por así convenir para este ejercicio"""

data_europa = data_europa.fillna(0)
data_vaccination = data_vaccination.fillna(0)
data_pib = data_pib.fillna(0)

data_europa.info()

data_vaccination.info()

data_pib.info()

"""### **Agrupación de la información**
Se obtiene un solo conjunto datos con las variables necesarias agrupadas por país.
"""

df1 = data_europa[['countriesAndTerritories', 'popData2020', 'cases', 'deaths', 'countryterritoryCode']]
df2 = data_vaccination[['COUNTRY', 'TOTAL_VACCINATIONS', 'NUMBER_VACCINES_TYPES_USED', 'ISO3']]
df3 = data_pib[['Country Name','2021','Country Code']]

df1.rename(columns = {'countriesAndTerritories':'pais', 'popData2020':'poblacion', 'cases':'casos', 'deaths':'muertes', 'countryterritoryCode':'codigo'} , inplace = True)
df2.rename(columns = {'COUNTRY': 'pais', 'TOTAL_VACCINATIONS': 'total_vacunados', 'NUMBER_VACCINES_TYPES_USED': 'cantidad_por_tipo_vacuna', 'ISO3':'codigo'}, inplace = True)
df3.rename(columns = {'Country Name': 'pais', '2021': 'pib', 'Country Code':'codigo'}, inplace = True)

df1 = df1.groupby(by=["codigo"]).sum()
df2 = df2.groupby(by=["codigo"]).sum()

data_pais = pd.merge(df1, df2, on='codigo')
data_pais = pd.merge(data_pais, df3, on='codigo')

data_pais.info()

data_pais = data_pais.set_index('pais')

data_pais = data_pais.drop(['codigo'],axis=1)

"""Conjunto de datos finales para la clusterización"""

data_pais

"""### **Estandarización de variables**

Es necesario tener las variables en una misma escala para obtener mejores resultados
"""

ss = StandardScaler()
data_pais_ss = ss.fit_transform(data_pais)

data_pais_df = pd.DataFrame(data_pais_ss, columns=['poblacion', 'casos', 'muertes', 'total_vacunados', 'cantidad_por_tipo_vacuna','pib'])

data_pais_df

"""### **Análisis de Componentes Principales**

Es una técnica utilizada para describir un conjunto de datos en términos de nuevas variables (componentes) no correlacionadas. Es particularmente útil para reducir la dimensionalidad de un grupo de datos.
"""

pca = PCA()
pca.fit(data_pais_ss)

features = range(pca.n_components_)

fig = plt.figure(figsize = (8,6))
plt.bar(features, pca.explained_variance_ratio_ * 100, color = 'purple', alpha = 0.5)
plt.suptitle('Análisis de Componentes Principales' , fontsize = 20)
plt.title('Descriptores de Covid-19', fontsize = 18)
plt.xlabel('Componentes', fontsize = 15)
plt.ylabel('Varianza explicada (%)', fontsize = 15)
plt.xticks(features, fontsize = 12)
plt.yticks([i * 10 for i in range(1, 10)], fontsize = 12)
plt.axhline(y = 10, linestyle = '--', color = 'black')
plt.show()

"""Con los resultados mostrados podemos concluir que los dos primeros componentes describen la mayor parte de la varianza, más del 90% """

pca = PCA(n_components = 2)

# Ajustamos y transformamos los datos
pca_features = pca.fit_transform(data_pais_ss)
print(pca_features)
print(pca_features.shape)

pca_components = pca.components_
print(pca_components.T)
print(pca_components.shape)

#Asignamos los componentes a dos variables
xs = pca_features[:,0]
ys = pca_features[:,1]

#Extraemos las etiquetas de los paises
pais = data_pais.index
pais = np.asarray(pais)

# Generemos el diagrama de dispersión
plt.figure(figsize=(10, 10))
plt.scatter(xs, ys)

for i, n in enumerate(pais):
    plt.annotate(n, (xs[i], ys[i]))
    
plt.title('Dispersión de los paises', fontsize = 18)
plt.xlabel('Componente 1', fontsize = 15)
plt.ylabel('Componente 2', fontsize = 15)
plt.xticks(fontsize = 12)
plt.yticks(fontsize = 12)
plt.axhline(y = 0, linestyle = '--', color = 'black')
plt.axvline(x = 0, linestyle = '--', color = 'black')
plt.show()

"""Se agregan los 2 componentes al dataset inicial"""

data_pais[['pca1', 'pca2']] = pca_features

data_pais

"""### **Clusterización K-Means**

Probar con distintos números de clústeres y comparar su inercia
"""

ks = range(1, 11)
inertias = []

for k in ks:
    model = KMeans(n_clusters = k)
    model.fit(pca_features)
    inertias.append(model.inertia_)

"""Gráfico de sedimentación para ver las incercias"""

fig = plt.figure(figsize = (8, 6))
plt.plot(ks, inertias, '-o', color = 'orange')
plt.title('Gráfica de sedimentación' , fontsize = 20)
plt.xlabel('Número de clústeres, k', fontsize = 15)
plt.ylabel('Inercia', fontsize = 15)
plt.xticks(ks, fontsize = 12)
plt.yticks(fontsize = 12)
plt.show()

"""Parece que después de 4 o 5 clústeres la inercia ya no se reduce tan rápido ... vamos a probar con 4 clústeres"""

# Generamos nuestra instancia con 4 clústeres
model = KMeans(n_clusters = 4)

# Ajustamos el modelo a nuestros datos y predecimos los centroides
clusters = model.fit_predict(pca_features)
print(clusters)

# Obtenemos los centroides y separamos sus coordenadas
centroides = model.cluster_centers_

centroides_x = centroides[:,0]
centroides_y = centroides[:,1]

# Observemos nuevamente nuestro gráfico de dispersión con la clusterización de los paises
plt.figure(figsize = (15, 15))

plt.scatter(xs, ys, c = clusters)
plt.scatter(centroides_x, centroides_y, marker="D", s=50, color = 'darkred')

for i, n in enumerate(pais):
    plt.annotate(n, (xs[i], ys[i]))

plt.title('Dispersión de diagnosticos - K-Means', fontsize = 18)
plt.xlabel('Componente 1', fontsize = 15)
plt.ylabel('Componente 2', fontsize = 15)
plt.xticks(fontsize = 12)
plt.yticks(fontsize = 12)
plt.axhline(y = ys.mean(), linestyle = '--', color = 'black')
plt.axvline(x = xs.mean(), linestyle = '--', color = 'black')
plt.show()

"""Obtener los centriodes"""

centroides

cen_x = [i[0] for i in centroides] 
cen_y = [i[1] for i in centroides]

"""Agregamos la informacion al dataset inicial"""

data_pais['cluster'] = clusters
#data_pais['cen_x'] = data_pais.cluster.map({0:cen_x[0], 1:cen_x[1]})
#data_pais['cen_y'] = data_pais.cluster.map({0:cen_y[0], 1:cen_y[1]})

data_pais['cen_x'] = 0
data_pais['cen_y'] = 0

data_pais.cen_x[data_pais.cluster==0] = cen_x[0]
data_pais.cen_x[data_pais.cluster==1] = cen_x[1]
data_pais.cen_x[data_pais.cluster==2] = cen_x[2]
data_pais.cen_x[data_pais.cluster==3] = cen_x[3]

data_pais.cen_y[data_pais.cluster==0] = cen_y[0]
data_pais.cen_y[data_pais.cluster==1] = cen_y[1]
data_pais.cen_y[data_pais.cluster==2] = cen_y[2]
data_pais.cen_y[data_pais.cluster==3] = cen_y[3]

print(data_pais[['cen_x', 'cen_y', 'cluster']].head())

colors = ['#551A8B', '#436EEE', '#2E8B57', '#EE5C42', '#FF0000', '#00FF00']
data_pais['c'] = data_pais.cluster.map({0:colors[0], 1:colors[1], 2:colors[2], 3:colors[3], 4:colors[4], 5:colors[5]})

plt.figure(figsize = (15, 15))

plt.scatter(data_pais['pca1'], data_pais['pca2'], c = data_pais['c'], alpha = 0.6)
plt.scatter(data_pais['cen_x'], data_pais['cen_y'], marker="D", s=70, color = data_pais['c'])

for idx, val in data_pais.iterrows():
    x = [val['pca1'], val['cen_x']]
    y = [val['pca2'], val['cen_y']]
    plt.plot(x, y, color = val['c'], alpha = 0.4)

cent_leg = [Line2D([0], [0], marker='^', color='w', label='Centroid - C{}'.format(i+1), 
            markerfacecolor=mcolor, markersize=10) for i, mcolor in enumerate(colors)]
plt.legend(handles=cent_leg, loc='upper right', ncol=2)

for i, n in enumerate(pais):
    plt.annotate(n, (xs[i], ys[i]))
    
plt.title('Dispersión de los paises - K-Means', fontsize = 18)
plt.xlabel('Componente 1', fontsize = 15)
plt.ylabel('Componente 2', fontsize = 15)
plt.xticks(fontsize = 12)
plt.yticks(fontsize = 12)
plt.show()

data_pais

"""### **Clustering Aglomerativo**"""

from sklearn.cluster import AgglomerativeClustering
ModeloAC = AgglomerativeClustering(n_clusters=4).fit(pca_features)

# Obtenemos los centroides y separamos sus coordenadas
clustersAC = ModeloAC.labels_

# Observemos nuevamente nuestro gráfico de dispersión con la clusterización de los paises
plt.figure(figsize = (15, 15))

plt.scatter(xs, ys, c = clustersAC)

for i, n in enumerate(pais):
    plt.annotate(n, (xs[i], ys[i]))
    
plt.title('Dispersión de diagnosticos - Modelo AgglomerativeClustering', fontsize = 18)
plt.xlabel('Componente 1', fontsize = 15)
plt.ylabel('Componente 2', fontsize = 15)
plt.xticks(fontsize = 12)
plt.yticks(fontsize = 12)
plt.axhline(y = ys.mean(), linestyle = '--', color = 'black')
plt.axvline(x = xs.mean(), linestyle = '--', color = 'black')
plt.show()